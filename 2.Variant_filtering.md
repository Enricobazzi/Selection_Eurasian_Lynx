---
title: "Variant_filtering"
author: "Enrico"
date: "16 March 2020"
output: html_document
---

Next step in the pipeline is to filter unwanted variants from the final VCF file.

This are the criteria I will use to filter variants:

(1) Repetitive/Low mappability regions
(2) Indels + Non-biallelic sites
(3) Lynx lynx exclusive substitutions (frequency = 1 in all populations)
(4,5) Hard quality filters, as of GATK standard practices
(6) Under-represented, excessively missing variants
(7) Over/Under covered regions

I will write a script for the first 5 steps which don't require calculations of any filtering thresholds (they are the same no matter the database).
The script is: 2.Variant_filtering-executables/variant_filtering_1to5.sh
```
sbatch variant_filtering_1to5.sh ll_wholegenome_LyCa_ref.sorted
# Submitted batch job 3833293
```
For the last two steps (6 and 7), I will have to write separate script to fine tune my filtering thresholds based on the shape of my data.

To filter out excessively missing variants we have decided the following criteria: we will keep only variants which are represented in ALL populations by at LEAST four individuals. This is because we will be working with allele frequencies and less than four individuals would be really not ideal when calculating allele frequency for a population. Also we would like comparing all populations (are you sure?).

To do this I will write a script that will first divide our VCF of all individuals, into per-population VCFs. I'll first generate a file with the list of all sample names.
```
grep "#CHROM" $LUSTRE/LL_selection/LyCaRef_vcfs/ll_wholegenome_LyCa_ref.sorted.filter5.vcf |
tr '\t' '\n' | grep "_ll_" | cut -d '_' -f1-4 | sort -u \
> $LUSTRE/LL_selection/LyCaRef_bams/all-samples.namelist
```
From this file, I can generate a file with a list of names of each population with a loop. Because Mongolian lynxes are divided into 3 population names (ka,og,to) and I want them as a unique population (mo), I will make an if statement to create a single file for them. The list of rest of the individuals of each population is extracted normally.
```
popARRAY=($(cat $LUSTRE/LL_selection/LyCaRef_bams/all-samples.namelist | cut -d '_' -f3 | sort -u))

for pop in ${popARRAY[@]}
  do
   if [[ ${pop} == ka || ${pop} == og || ${pop} == to ]]
    then
    echo "extracting ${pop} names into mo.namelist"
    grep ${pop} $LUSTRE/LL_selection/LyCaRef_bams/all-samples.namelist \
    >> $LUSTRE/LL_selection/LyCaRef_bams/mo.namelist
   else
    echo "extracting ${pop} names into ${pop}.namelist"
    grep ${pop} $LUSTRE/LL_selection/LyCaRef_bams/all-samples.namelist \
    > $LUSTRE/LL_selection/LyCaRef_bams/${pop}.namelist
   fi
done
```
Then I can use these files to give the names of the samples I want to extract from the VCF file to BCFtools. So I will use BCFtools view in a loop for each population to generate a per-population VCF.

With the per-population VCF, I can again use BCFtools again to filter out variants with missing genotypes for too many samples. By telling BCFtools to include only those variants (-i), the output can be used as a list of positions to exclude from the original VCF.

The script (2.Variant_Filtering-executables/variant_filtering_6.sh) will be launched on CESGA's FT2 server.
```
./variant_filtering_6.sh ll_wholegenome_LyCa_ref.sorted
```

To filter out variants based on alignment depth I will proceed as follows:
  (1) generate a random subset of the reference genome of 200 windows of 100000 bp (representative but not too big)
  (2) calculate depth at each site for this subset for each population
  (3) draw the distribution of depth values and decide upper and lower limits of depth
  (4) find the SNPs in each population that fall outside of the decided limits
  (5) filter out from the whole dataset the SNPs from step 4

To generate a random subset of the reference genome I will use BEDtools random.
```
module load bedtools

# Create a Genome region file for Bedtools:
# A file with the list of chromosomes as col1 and their length as col2, tab separated
# Basically the first two columns of a FAI file:
cut -f1,2 $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.fa.fai > \
$LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.genome

# Bedtools random to generate file of 200 random segments of 100000 bp
# Output a BED file:
bedtools random -l 100000 -n 200 -g $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.genome | \
sort > $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.200x100kbp.genome.bed

# I also want to remove the repetetive regions from this subset, as depth at those
# positions doesn't matter and might bias our final calculations
bedtools subtract -a $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.200x100kbp.genome.bed \
-b $LUSTRE/LL_selection/Ref_Genome_LyCa/repetitive_regions/lc_rep_ALL_scaffold_coord.bed \
> $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.200x100kbp.masked.genome.bed
```
Now I can calculate, for each population, the depth in the random regions of the individuals of that population using samtools depth.

It has to be noted that in a few populations (ki, cr, ya, vl, ba, ca), there are individuals of higher average depth (c_ll_cr_0212, c_ll_ki_0090, c_ll_vl_0112, c_ll_ya_0146, c_ll_ba_0224, c_ll_ca_0240), because they were sequenced with a higher depth target. Because of this, I will exclude them from the analysis, as they will bias the depth distribution (especially for lower limit, as they might "carry" a variant which has very low coverage in the rest of the population).

Because samtools depth needs a file with the list of bams to use I will generate a depth.bamlist file for each population.
```
# Array of populations except mongolia
nomopopARRAY=($(ls $LUSTRE/LL_selection/LyCaRef_bams/*.namelist | rev | cut -d'/' -f1 | rev | grep -v "all-samples" | cut -d '.' -f1 | sort -u | grep -v "mo"))
# Generate population bamlist except mongolia
for pop in ${nomopopARRAY[@]}
  do
  echo "generating bamlist of ${pop}"
  ls $LUSTRE/LL_selection/LyCaRef_bams/*${pop}*_indelrealigner.bam |
  grep -vE "c_ll_cr_0212|c_ll_ki_0090|c_ll_vl_0112|c_ll_ya_0146|c_ll_ba_0224|c_ll_ca_0240" \
  > $LUSTRE/LL_selection/LyCaRef_bams/${pop}.depth.bamlist
done

# Generate mongolia bamlist
mopopARRAY=($(cat $LUSTRE/LL_selection/LyCaRef_bams/mo.namelist | cut -d'_' -f3 | sort -u))
for pop in ${mopopARRAY[@]}
  do
  echo "generating bamlist of mongolia - adding ${pop}"
  ls $LUSTRE/LL_selection/LyCaRef_bams/*${pop}*_indelrealigner.bam |
  grep -vE "c_ll_cr_0212|c_ll_ki_0090|c_ll_vl_0112|c_ll_ya_0146|c_ll_ba_0224|c_ll_ca_0240" \
  >> $LUSTRE/LL_selection/LyCaRef_bams/mo.depth.bamlist
done
```
Now that I have the lists of bams to include for each population, I can run samtools depth for each. I'll try doing and running everything at the same time (& at the end of the loop).
```
compute -c 15 --mem 10

module load samtools

# Array of bamlist files
BAMlistArray=($(ls $LUSTRE/LL_selection/LyCaRef_bams/*.depth.bamlist | rev | cut -d'/' -f 1 | rev))

# Loop of Samtools depth calculations for each bamlist
for bamlist in ${BAMlistArray[@]}
  do
  echo "Calculating depth for ${bamlist}"
  samtools depth -a -b $LUSTRE/LL_selection/Ref_Genome_LyCa/lc4.200x100kbp.masked.genome.bed \
  -f $LUSTRE/LL_selection/LyCaRef_bams/${bamlist} \
  > $LUSTRE/LL_selection/SamTools_Depth/${bamlist}.200x100kbp.masked.depth &
done
```
Once the files are generated I can download them on my laptop:
```
scp -r csebdjg2@ft2.cesga.es:/mnt/lustre/scratch/home/csic/ebd/jg2/LL_selection/SamTools_Depth/ Documents/Selection_Eurasian_Lynx/
```
and analyze them using R in R-Studio.

First I will load the libraries needed:
```{R}
library(readr)
library(dplyr)
library(ggplot2)
```
Then define input directory and input data files:
```{R}
# Define the input Directory
wd_input <- "/Users/enricobazzicalupo/Documents/Selection_Eurasian_Lynx/SamTools_Depth/"

# Create a list of the sample files' names (the SAMTOOLS depth output files)
sample_files <- list.files(wd_input, pattern="*.depth$")
```
I will then create an empty dataframe to store the calculated values for each population. With a loop through each population I will extract frequency of depth values, define upper and lower limits of depth values, generate a row of the population to add to the general dataframe, draw the distribution and the limits (for visualization purposes).
```{R}
# Create an Empty dataframe to save values for each dataset for the final table
depth_per_sample <- data.frame()

# For every Sample file:
for (i in 1:length(sample_files)){

  # Import the sample file table
  input.depth <- read_delim(paste0(wd_input,sample_files[[i]]), col_names = F, delim = '\t')

  # Add a column (Total) which is the sum of all the depth columns (from the third to the last)
  input.depth$Total <- rowSums(input.depth[,3:ncol(input.depth)])

  # Create a frequency table of the values of the Total column
  freq_table_DF <- as.data.frame(table(input.depth$Total))

  # Define Dataset Name:
  population=unlist(strsplit(basename(sample_files[[i]]),"[.]"))[1]

  # Define the functions for mean and standard deviation,
  # and define maximum and minimum depth based on population:
  # minimum of 5x per individual is needed for "medium depth" populations (ca,ba)
  # and of 3x per individual for "low depth" populations (the rest)

  if (population=="ba" || population=="ca") {

   mymean <- mean(input.depth$Total)
   mysd <- sd(input.depth$Total)
   maxDepth_sd = mymean + (mysd * 1.5)
   minDepth <- 5 * (ncol(input.depth) - 3)

  } else {

   mymean <- mean(input.depth$Total)
   mysd <- sd(input.depth$Total)
   maxDepth_sd = mymean + (mysd * 1.5)
   minDepth <- 3 * (ncol(input.depth) - 3)

  }


  basename(sample_files[[i]])
  # Add dataset information to Dataframe
  depth_per_sample <- rbind(depth_per_sample,
                            data.frame(pop = population, nindividuals = (ncol(input.depth) - 3),
                                       mean = mymean, sd = mysd,
                                       maxDepthSD = maxDepth_sd, minDepth3x = minDepth))

# Draw and save a graph of the distribution of depth values, with upper and lower depth limits
 ggplot(freq_table_DF, aes(x = as.numeric(Var1), y = Freq)) +
   geom_bar(stat = "identity", color = "black") +
   scale_x_continuous(breaks = 0:250*10, limits = c(0, maxDepth_sd*1.5)) +
   # scale_x_discrete(limits = c(0, maxDepth_DF*1.5)) +
   scale_y_continuous(expand=c(0,0)) +
   geom_vline(xintercept=maxDepth_sd,linetype="dashed", size=0.5) +
   geom_vline(xintercept=minDepth,linetype="dashed", size=0.5) +
   #geom_vline(xintercept=minDepth_DF_meanfolds, colour ="grey", linetype="dashed", size=0.5) +
   #geom_vline(xintercept=maxDepth_DF_meanfolds, colour ="grey", linetype="dashed", size=0.5) +
   #geom_vline(xintercept=minDepth_5x, colour ="red", linetype="dashed", size=0.5) +
   #geom_vline(xintercept=maxDepth_MEAN, colour ="red", linetype="dashed", size=0.5) +
   theme_classic() +
   theme(text = element_text(size=10))
 ggsave (filename = (paste0("graph_",basename(sample_files[[i]]),".pdf")), path = wd_input)
}
```
Having extracted the values we will use, we can output the dataframe to a csv file:
```{R}
# Print the table to a file
write.table(x = depth_per_sample,file = paste0(wd_input,"depth_per_sample.csv"),
            quote=FALSE, col.names = T, row.names = FALSE, sep= ",")
```
and copy that table to the FT2 CESGA server to use its values to filter our VCF file
```
scp /Users/enricobazzicalupo/Documents/Selection_Eurasian_Lynx/SamTools_Depth/depth_per_sample.csv \
csebdjg2@ft2.cesga.es:/mnt/lustre/scratch/home/csic/ebd/jg2/LL_selection/SamTools_Depth/
```
Now I can filter out the variants that fall outside the calculated thresholds.

I will first use GATK SelectVariants to divide my whole dataset VCF into a per-population VCF, excluding the individuals of "high" depth from the populations that have them (c_ll_cr_0212, c_ll_ki_0090, c_ll_vl_0112, c_ll_ya_0146, c_ll_ba_0224, c_ll_ca_0240). Then with bcftools filter I can extract the variants that fall outside of my thresholds. Finally with bedtools subtract I will remove those variants from the whole dataset VCF.

The script (2.Variant_Filtering-executables/variant_filtering_7.sh) will be launched on CESGA's FT2 server.
```
sbatch variant_filtering_7.sh ll_wholegenome_LyCa_ref.sorted
# Submitted batch job 3852240
```
